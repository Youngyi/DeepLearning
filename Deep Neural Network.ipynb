{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 深度L层神经网络\n",
    "\n",
    "之前我们已经看过了逻辑回归（没有隐藏层，只包含输入层和输出层，可以认为是单层神经网络），包含一层隐藏层的“浅层神经网络”（两层神经网络，计算层数时，不包含输入层）。而所谓深度神经网络，就是增加隐藏层的数量。\n",
    "\n",
    "这些年AI/机器学习社区的实践发现，诸如图像、自然语言在内的一些问题，需要深度神经网络才可能学到相应的模式，而浅层的模型完全无法做到。但同时，我们无法提前预知，处理这些问题需要多深的神经网络。因为深度神经网络的层数、每层神经元的数量都可以看做是超参，需要通过交叉验证之类的方法来确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. 深度神经网络中的前向传播\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "for \\, l &= 1, 2, ..., L: \\\\\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 确定深度神经网络中各矩阵的维度\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "W^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "b^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "dW^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "db^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "Z^{[l]},A^{[l]} &: (n^{[l]}, m) \\\\\n",
    "dZ^{[l]},dA^{[l]} &: (n^{[l]}, m) \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 为什么要用深度神经网路\n",
    "\n",
    "深度神经网络：逐层增加复杂度。\n",
    "\n",
    "- 人脸识别问题：检测边界 -> 检测器官 -> 检测面部；\n",
    "\n",
    "- 语音识别问题：底层的特征（比如音调上升或下降，白噪声等） -> 声音的基本单元（音位Phoneme） -> 单词 -> 句子；\n",
    "\n",
    "神经科学认为，人类大脑的工作流程也是从简单的检测功能构建起来，进而检测更复杂的内容。\n",
    "\n",
    "![Intuition about deep representation](img/Intuition about deep representation.png)\n",
    "\n",
    "电路理论和深度学习：要用浅层的神经网络实现深度神经网络同样的功能，需要浅层神经网络中具备相比深度神经网络指数级别多的神经元。\n",
    "\n",
    "- 一个例子：多个输入变量的XOR，深度神经网络需要 $O(logn)$ 级别的神经元；而双层（单隐藏层）神经网络需要 $O(2^n)$ 级别的神经元\n",
    "\n",
    "![Circuit theory and deep learning](img/Circuit theory and deep learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 深度神经网络的组成\n",
    "\n",
    "前向传播和后向传播，计算前向传播的过程中，缓存相应的变量，用于后续计算后向传播。\n",
    "\n",
    "![Forward and backward functions](img/Forward and backward functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
