{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 深度L层神经网络\n",
    "\n",
    "之前我们已经看过了逻辑回归（没有隐藏层，只包含输入层和输出层，可以认为是单层神经网络），包含一层隐藏层的“浅层神经网络”（两层神经网络，计算层数时，不包含输入层）。而所谓深度神经网络，就是增加隐藏层的数量。\n",
    "\n",
    "这些年AI/机器学习社区的实践发现，诸如图像、自然语言在内的一些问题，需要深度神经网络才可能学到相应的模式，而浅层的模型完全无法做到。但同时，我们无法提前预知，处理这些问题需要多深的神经网络。因为深度神经网络的层数、每层神经元的数量都可以看做是超参，需要通过交叉验证之类的方法来确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. 深度神经网络中的前向传播\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "for \\, l &= 1, 2, ..., L: \\\\\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 确定深度神经网络中各矩阵的维度\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "W^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "b^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "dW^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "db^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "Z^{[l]},A^{[l]} &: (n^{[l]}, m) \\\\\n",
    "dZ^{[l]},dA^{[l]} &: (n^{[l]}, m) \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 为什么要用深度神经网路\n",
    "\n",
    "深度神经网络：逐层增加复杂度。\n",
    "\n",
    "- 人脸识别问题：检测边界 -> 检测器官 -> 检测面部；\n",
    "\n",
    "- 语音识别问题：底层的特征（比如音调上升或下降，白噪声等） -> 声音的基本单元（音位Phoneme） -> 单词 -> 句子；\n",
    "\n",
    "神经科学认为，人类大脑的工作流程也是从简单的检测功能构建起来，进而检测更复杂的内容。\n",
    "\n",
    "![Intuition about deep representation](img/Intuition about deep representation.png)\n",
    "\n",
    "电路理论和深度学习：要用浅层的神经网络实现深度神经网络同样的功能，需要浅层神经网络中具备相比深度神经网络指数级别多的神经元。\n",
    "\n",
    "- 一个例子：多个输入变量的XOR，深度神经网络需要 $O(logn)$ 级别的神经元；而双层（单隐藏层）神经网络需要 $O(2^n)$ 级别的神经元\n",
    "\n",
    "![Circuit theory and deep learning](img/Circuit theory and deep learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 深度神经网络的组成\n",
    "\n",
    "前向传播反向传播，计算前向传播的过程中，缓存相应的变量，用于后续计算反向传播。\n",
    "\n",
    "![Forward and backward functions](img/Forward and backward functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 前向传播和反向传播\n",
    "\n",
    "反向传播过程：\n",
    "$$\n",
    "\\begin{split}\n",
    "for \\, l &= L, L-1, ..., 1: \\\\\n",
    "dZ^{[l]} &= dA^{[l]} * g^{[l]\\prime}(Z^{[l]}) \\\\\n",
    "dW^{[l]} &= \\frac{1}{m}dZ^{[l]} \\cdot A^{[l-1]T} \\\\\n",
    "db^{[l]} &= \\frac{1}{m}np.sum(dZ{[l]}, axis=1, keepdims=True) \\\\\n",
    "dA^{[l-1]} &= W^{[l]T} \\cdot dZ^{[l]}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "![Summary](img/Summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 参数和超参\n",
    "\n",
    "这部分和传统机器学习差别不大，唯一的区别可能是深度学习的超参相对更多。超参需要通过交叉验证来调。\n",
    "\n",
    "![What are hyperparameters](img/What are hyperparameters.png)\n",
    "\n",
    "应用深度学习是一个经验性的过程，意思是说，调节超参的过程，存在很多重复尝试的阶段。暂时也没有什么好的方法，预先知道超参的大致范围。\n",
    "![Applied deep learning is a very empirical process](img/Applied deep learning is a very empirical process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. 人工神经网络和大脑的关系\n",
    "\n",
    "人工神经网络的灵感来自于大脑基于无数神经元的结构。但事实上，神经科学至今也还不能很好地解释大脑的运作原理，大脑学习的过程也可能会和人工神经网络的激活、反向传播截然不同。而深度学习发展到今天，和当初来自神经元的灵感也已经有很大不同了，二者可能并不存在太多实际的关联。\n",
    "\n",
    "把人工神经网络，尤其是深度学习，看做是一种拟合X->Y关系的高效机器学习算法就好。这可能是目前最合适的定位。\n",
    "\n",
    "![What does this have to do with the brain](img/What does this have to do with the brain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Show Me The Code\n",
    "\n",
    "之前我们构建了2层的神经网络，接下来我们将构建可以包含任意多层的深度神经网络。\n",
    "\n",
    "**通过这个编程练习，可以学到：**\n",
    "- 使用诸如ReLU的非线性神经元来提升模型\n",
    "- 构建更深层的神经网络（超过一个隐藏层）\n",
    "- 实现一个易用的神经网络类\n",
    "\n",
    "**标记**:\n",
    "- 上标 $[l]$ 表示第 $l^{th}$ 层相关的数据 \n",
    "    - 举例：$a^{[L]}$ 是第 $L^{th}$ 层的激活值. $W^{[L]}$ 和 $b^{[L]}$ 是第 $L^{th}$ 层的参数。\n",
    "- 上标 $(i)$ 表示第 $i^{th}$ 个样本相关的数据\n",
    "    - 举例：$x^{(i)}$ 是第 $i^{th}$ 个训练样本。\n",
    "- 下标 $i$ 表示向量中的第 $i^{th}$ 个值。\n",
    "    - 举例：$a^{[l]}_i$ 表示第 $l^{th}$ 层的第 $i^{th}$ 个激活值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 三方包\n",
    "\n",
    "首先，运行下面的代码块，来引入在这个编程练习中所需要的包。 \n",
    "- [numpy](www.numpy.org) 是Python生态圈中进行科学计算的基础包。\n",
    "- [matplotlib](http://matplotlib.org) 是Python生态圈中著名的绘图包。\n",
    "- dnn_utils 提供了一些辅助函数\n",
    "- testCases 提供了一些测试用力，用来测试所写函数的准确性\n",
    "- np.random.seed(1) 用来保证随机函数调用结果的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9.2 问题总览\n",
    "\n",
    "为了自行构建神经网络，需要首先实现一些“辅助函数”。这些辅助函数后面会用于构建一个两层的神经网络和一个L层的神经网络，主要包括如下：\n",
    "\n",
    "- 为两层神经网络和 $L$ 层神经网络初始化参数\n",
    "- 实现前向传播（图中紫色的部分）\n",
    "     - 完成前向传播步骤中线性组合的部分（输出 $Z^{[l]}$）\n",
    "     - 使用已经给定了激活函数（relu/sigmoid）\n",
    "     - 将上面两步结合为一个前向传播函数（LINEAR->ACTIVATION）\n",
    "     - 计算 [LINEAR->RELU] 前向传播 L-1 次（从第1层到第L-1层），最后再计算一次 [LINEAR->SIGMOID]（第L层）。合并这两步，得到 L_model_forward 函数。\n",
    "- 计算损失\n",
    "- 实现反向传播（图中红色的部分）\n",
    "     - 完成反向传播步骤中线性的部分\n",
    "     - 使用已经给定了的激活函数的梯度（relu_backward/sigmoid_backward）\n",
    "     - 将上面两步结合为一个反向传播函数（LINEAR->ACTIVATION）\n",
    "     - 计算 [LINEAR->RELU] 反向传播 L-1 次，最后再计算一次 [LINEAR->SIGMOID]。合并这两步，得到 L_model_backward 函数。\n",
    "- 更新参数\n",
    "\n",
    "<img src=\"img/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **图 1**</center></caption><br>\n",
    "\n",
    "**注意** 对于每个前向传播函数，都有对应的反向传播函数。因此每一步前向传播模块都要保存一些值到缓存中。缓存中的值可以用于反向传播模块中计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 初始化\n",
    "\n",
    "我们需要写两个辅助函数用于模型参数的初始化，分别对应两层神经网络和 $L$ 层神经网络。\n",
    "\n",
    "#### 9.3.1 两层神经网络\n",
    "\n",
    "**练习**: 创建一个两层神经网络并初始化参数。\n",
    "\n",
    "**指令**:\n",
    "- 模型的结构为：*LINEAR -> RELU -> LINEAR -> SIGMOID*\n",
    "- 权重矩阵需要随机初始化。用正确的矩阵维度调用 `np.random.randn(shape)*0.01`\n",
    "- 截距向量使用0值初始化。调用 `np.zeros(shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2 L层神经网络\n",
    "\n",
    "L层深度神经网络的初始化会稍微复杂一些，因为我们现在有更多的权重矩阵和截距向量。在实现 `initialize_parameters_deep` 时，你需要保证每一层相关参数的维度正确。回忆一下， $n^{[l]}$ 表示 $l$ 层的神经元数量。因此假设输入 $X$ 是 $(12288, 209)$ 维度的矩阵时（包含 $m=209$ 个训练样本），那么：\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "回顾一下，当我们使用numpy计算 $W X + b$ 时，它会使用广播机制。例如，如果： \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "那么 $WX + b$ 的结果是：\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 实现L层神经网络的参数初始化\n",
    "\n",
    "**指令**：\n",
    "- 模型的结构为 *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*。也就是说，前 $L-1$ 层使用ReLU激活函数，而输出层使用sigmoid激活函数。\n",
    "- 权重矩阵使用随机初始化。调用 `np.random.rand(shape) * 0.01`。\n",
    "- 截距向量使用0值初始化。调用 `np.zeros(shape)`。\n",
    "- 我们会在变量 `layer_dims` 中保存 $n^{[l]}$，也即不同层神经元的数量。\n",
    "- 下面是 $L=1$ 时的实现。可以根据这个例子推广到 $L$ 为任意正正数的情况\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 前向传播模块\n",
    "\n",
    "#### 9.4.1 线性前向传播 \n",
    "有了初始化的参数之后，就可以开始开发前向传播的模块。首先我们需要按顺序实现一些基本的函数：\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION 其中激活函数可以是 ReLU 或 Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID 整个前向传播过程\n",
    "\n",
    "前向传播模块的计算公式是：\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "其中 $A^{[0]} = X$. \n",
    "\n",
    "**练习**: 构建前向传播的线性部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9.4.2 线性-激活 前向传播\n",
    "\n",
    "在这个notebook中，我们将会使用两种激活函数：\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$。我们已经引入了 `sigmoid` 函数。这个函数返回 **两个** 变量：激活值 \"`a`\" 和缓存 \"`cache`\"，缓存中包含了 \"`Z`\" (在反向传播中会用到)。按照下面的方法调用函数即可\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: ReLU的数学公式是 $A = RELU(Z) = max(0, Z)$。同样的， `relu` 函数已经在上面引入了，这个函数也返回 **两个** 变量：激活值 \"`a`\" 和缓存 \"`cache`\"，缓存中包含了 \"`Z`\" (在反向传播中会用到)。按照下面的方法调用函数即可\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用方便，我们会将线性组合函数和激活函数组合成一个线性-激活函数。因此，下面的实现会首先进行线性组合，接着进行激活。\n",
    "\n",
    "**练习**：实现 *LINEAR->ACTIVATION* 的前向传播过程。数学公式为：A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ 其中激活函数 \"g\" 可以是 sigmoid() 或 relu()。调用 linear_forward() 以及相应的激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[ 0.96890023  0.11013289]]\n",
      "With ReLU: A = [[ 3.43896131  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：在深度学习中，\"[LINEAR->ACTIVATION]\" 的计算过程是神经网络中的一层，而不是两层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4.3 L层的模型\n",
    "\n",
    "为了L层神经网络模型的使用便利，我们需要一个函数重复上面的步骤（使用ReLU的 `linear_activation_forward`） $L-1$ 次，之后再计算一次使用SIGMOID的 `linear_activation_forward`。\n",
    "\n",
    "<img src=\"img/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **图 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "**练习**：实现上面这个模型的前向传播。\n",
    "\n",
    "**指令**：在上面的代码块中，变量 `AL` 表示 $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (有时也命名为 `Yhat`，即 $\\hat{Y}$) \n",
    "\n",
    "**提示**:\n",
    "- 使用上面实现过的函数 \n",
    "- 重复 [LINEAR->RELU] (L-1) 次的过程可以使用循环\n",
    "- 将每层计算产生的缓存保存在 \"caches\" 列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，这样我们就给定输入 X 可以获得作为输出的一个行向量 $A^{[L]}$，这个向量中包含了模型的预测值。同时，所有的中间结果保存在 \"caches\" 变量中。使用 $A^{[L]}$ 可以计算预测的成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
