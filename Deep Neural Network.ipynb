{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 深度L层神经网络\n",
    "\n",
    "之前我们已经看过了逻辑回归（没有隐藏层，只包含输入层和输出层，可以认为是单层神经网络），包含一层隐藏层的“浅层神经网络”（两层神经网络，计算层数时，不包含输入层）。而所谓深度神经网络，就是增加隐藏层的数量。\n",
    "\n",
    "这些年AI/机器学习社区的实践发现，诸如图像、自然语言在内的一些问题，需要深度神经网络才可能学到相应的模式，而浅层的模型完全无法做到。但同时，我们无法提前预知，处理这些问题需要多深的神经网络。因为深度神经网络的层数、每层神经元的数量都可以看做是超参，需要通过交叉验证之类的方法来确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. 深度神经网络中的前向传播\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "for \\, l &= 1, 2, ..., L: \\\\\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 确定深度神经网络中各矩阵的维度\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "W^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "b^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "dW^{[l]} &: (n^{[l]}, n^{[l-1]}) \\\\\n",
    "db^{[l]} &: (n^{[l]}, 1) \\\\\n",
    "Z^{[l]},A^{[l]} &: (n^{[l]}, m) \\\\\n",
    "dZ^{[l]},dA^{[l]} &: (n^{[l]}, m) \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 为什么要用深度神经网路\n",
    "\n",
    "深度神经网络：逐层增加复杂度。\n",
    "\n",
    "- 人脸识别问题：检测边界 -> 检测器官 -> 检测面部；\n",
    "\n",
    "- 语音识别问题：底层的特征（比如音调上升或下降，白噪声等） -> 声音的基本单元（音位Phoneme） -> 单词 -> 句子；\n",
    "\n",
    "神经科学认为，人类大脑的工作流程也是从简单的检测功能构建起来，进而检测更复杂的内容。\n",
    "\n",
    "![Intuition about deep representation](img/Intuition about deep representation.png)\n",
    "\n",
    "电路理论和深度学习：要用浅层的神经网络实现深度神经网络同样的功能，需要浅层神经网络中具备相比深度神经网络指数级别多的神经元。\n",
    "\n",
    "- 一个例子：多个输入变量的XOR，深度神经网络需要 $O(logn)$ 级别的神经元；而双层（单隐藏层）神经网络需要 $O(2^n)$ 级别的神经元\n",
    "\n",
    "![Circuit theory and deep learning](img/Circuit theory and deep learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 深度神经网络的组成\n",
    "\n",
    "前向传播和后向传播，计算前向传播的过程中，缓存相应的变量，用于后续计算后向传播。\n",
    "\n",
    "![Forward and backward functions](img/Forward and backward functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 前向传播和后向传播\n",
    "\n",
    "后向传播过程：\n",
    "$$\n",
    "\\begin{split}\n",
    "for \\, l &= L, L-1, ..., 1: \\\\\n",
    "dZ^{[l]} &= dA^{[l]} * g^{[l]\\prime}(Z^{[l]}) \\\\\n",
    "dW^{[l]} &= \\frac{1}{m}dZ^{[l]} \\cdot A^{[l-1]T} \\\\\n",
    "db^{[l]} &= \\frac{1}{m}np.sum(dZ{[l]}, axis=1, keepdims=True) \\\\\n",
    "dA^{[l-1]} &= W^{[l]T} \\cdot dZ^{[l]}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "![Summary](img/Summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 参数和超参\n",
    "\n",
    "这部分和传统机器学习差别不大，唯一的区别可能是深度学习的超参相对更多。超参需要通过交叉验证来调。\n",
    "\n",
    "![What are hyperparameters](img/What are hyperparameters.png)\n",
    "\n",
    "应用深度学习是一个经验性的过程，意思是说，调节超参的过程，存在很多重复尝试的阶段。暂时也没有什么好的方法，预先知道超参的大致范围。\n",
    "![Applied deep learning is a very empirical process](img/Applied deep learning is a very empirical process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. 人工神经网络和大脑的关系\n",
    "\n",
    "人工神经网络的灵感来自于大脑基于无数神经元的结构。但事实上，神经科学至今也还不能很好地解释大脑的运作原理，大脑学习的过程也可能会和人工神经网络的激活、反向传播截然不同。而深度学习发展到今天，和当初来自神经元的灵感也已经有很大不同了，二者可能并不存在太多实际的关联。\n",
    "\n",
    "把人工神经网络，尤其是深度学习，看做是一种拟合X->Y关系的高效机器学习算法就好。这可能是目前最合适的定位。\n",
    "\n",
    "![What does this have to do with the brain](img/What does this have to do with the brain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Show Me The Code\n",
    "\n",
    "之前我们构建了2层的神经网络，接下来我们将构建可以包含任意多层的深度神经网络。\n",
    "\n",
    "**通过这个编程练习，可以学到：**\n",
    "- 使用诸如ReLU的非线性神经元来提升模型\n",
    "- 构建更深层的神经网络（超过一个隐藏层）\n",
    "- 实现一个易用的神经网络类\n",
    "\n",
    "**标记**:\n",
    "- 上标 $[l]$ 表示第 $l^{th}$ 层相关的数据 \n",
    "    - 举例：$a^{[L]}$ 是第 $L^{th}$ 层的激活值. $W^{[L]}$ 和 $b^{[L]}$ 是第 $L^{th}$ 层的参数。\n",
    "- 上标 $(i)$ 表示第 $i^{th}$ 个样本相关的数据\n",
    "    - 举例：$x^{(i)}$ 是第 $i^{th}$ 个训练样本。\n",
    "- 下标 $i$ 表示向量中的第 $i^{th}$ 个值。\n",
    "    - 举例：$a^{[l]}_i$ 表示第 $l^{th}$ 层的第 $i^{th}$ 个激活值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 三方包\n",
    "\n",
    "首先，运行下面的代码块，来引入在这个编程练习中所需要的包。 \n",
    "- [numpy](www.numpy.org) 是Python生态圈中进行科学计算的基础包。\n",
    "- [matplotlib](http://matplotlib.org) 是Python生态圈中著名的绘图包。\n",
    "- dnn_utils 提供了一些辅助函数\n",
    "- testCases 提供了一些测试用力，用来测试所写函数的准确性\n",
    "- np.random.seed(1) 用来保证随机函数调用结果的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
