{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 0. 深度学习的标记法\n",
    "\n",
    "### 0.1 神经网络的标记法\n",
    "\n",
    "**一般标记 General comments**：\n",
    "\n",
    "- 上标 $(i)$ 表示第 $i$ 个训练样本；上标 $[l]$ 表示第 $l$ 层。\n",
    "\n",
    "**大小 Sizes**:\n",
    "\n",
    "- $m$: 数据集中的样本数量\n",
    "\n",
    "- $n_x$: 输入大小（特征数）\n",
    "\n",
    "- $n_y$: 输出大小（分类的数量）\n",
    "\n",
    "- $n_h^{[l]}$: 第 $l$ 层的隐藏神经元数量\n",
    "\n",
    "- $L$: 神经网络的总层数\n",
    "\n",
    "这样，在for循环中，可以定义 $n_x = n_h^{[0]}, n_y = n_h^{[L+1]}$\n",
    "\n",
    "**对象 Objects**\n",
    "\n",
    "- $X \\in \\mathbb{R}^{n_x \\times m}$ 是输入矩阵\n",
    "\n",
    "- $x^{(i)} \\in \\mathbb{R}^{n_x}$ 是第 $i$ 个样本，表示为一个 $n_x$ 维的列向量\n",
    "\n",
    "- $Y \\in \\mathbb{R}^{n_y \\times m}$ 是标签矩阵\n",
    "\n",
    "- $y^{(i)} \\in \\mathbb{R}^{n_y}$ 是第 $i$ 个样本对应的标签，表示为一个 $n_y$ 维的列向量\n",
    "\n",
    "- $W^{[l]} \\in \\mathbb{R}^{下一层的神经元数量 \\times 上一层的神经元数量}$ 是权重矩阵，上标 $[l]$ 表示所在层\n",
    "\n",
    "- $b^{[l]} \\in \\mathbb{R}^{下一层的神经元数量}$ 是第 $l$ 层的截距向量\n",
    "\n",
    "- $\\hat{y} \\in \\mathbb{R}^{n_y}$ 是预测结果向量，也可以表示为 $a^{[L]}$\n",
    "\n",
    "**常见的前向传播公式**\n",
    "\n",
    "- $ a = g^{[l]}(W_xx^{(i)} + b_1) = g^{[l]}(z_1)$, $g^{[l]}$表示第l层的激活函数\n",
    "- $ \\hat{y}^{(i)} = softmax(W_hh + b_2) $\n",
    "- 一般的激活公式: $a_j^{[l]} = g^{[l]}(\\sum_k w_{jk}^{[l]}a_k^{[l-1]} + b_j^{[l]}) = g^{[l]}(z_j^{[l]}) $\n",
    "- $J(x,W,b,y)$ 或 $J(\\hat{y}, y)$ 表示成本函数\n",
    "\n",
    "**成本函数的例子**\n",
    "\n",
    "- $J_{CE}(\\hat{y}, y) = -\\sum_{i=0}^m y^{(i)}log\\hat{y}^{(i)}$\n",
    "- $J_1(\\hat{y}, y) = \\sum_{i=0}^m |y^{(i)} - \\hat{y}^{(i)}|$\n",
    "\n",
    "\n",
    "### 0.2 深度学习的标记法\n",
    "\n",
    "对于使用图结构来表示的神经网络\n",
    "\n",
    "- 节点表示输入、激活单元或输出\n",
    "- 变表示权重或截距\n",
    "\n",
    "![examples of Standard deep learning representations](img/examples of Standard deep learning representations.png)\n",
    "\n",
    "### 0.3 特别注意！\n",
    "\n",
    "为了计算的便利，神经网络和传统机器学习在表示层面上，有这样两个显著的不同：\n",
    "\n",
    "- 相对于传统机器学习，神经网络的输入矩阵 $X$ 是转置过的 $n \\times m$ 矩阵，其中每一列是一个样本，每一行是一个特征。传统机器学习习惯上 $X$ 是一个 $m \\times n$ 矩阵，其中每一行是一个样本，每一列是一个特征。相应的，标签矩阵 $y$ 也经过了转置。\n",
    "\n",
    "- 在传统机器学习中，每个样本作为输入向量通常会加入一个 $x_0=1$ 的变量，使得输入矩阵成为 $m \\times (n+1)$ 的矩阵，从而输入向量和权重相乘的计算可以写作 $\\hat{y} = \\theta^Tx$。神经网络中通常将截距项和权重项分开，等同于 $b=\\theta_0$ 而 $w = [\\theta_1;\\theta_2;...;\\theta_n]$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. 二分类\n",
    "\n",
    "在二分类问题中，输出结果是离散型的二值。\n",
    "\n",
    "例子：判定图片里是否包含猫\n",
    "\n",
    "这里的目标是训练一个分类器，将图像作为输入并表示为特征向量 $x$，预测对应的标签 $y$ 是1还是0。在这里0、1的含义是这张图片包含猫（1）或不包含猫（0）。\n",
    "\n",
    "![image to feature vector](img/image to feature vector.png)\n",
    "\n",
    "在计算机中图像通常被存储成三个独立的矩阵，分别对应着图像的红、绿、蓝三个通道。三个矩阵和图片的像素大小相同，比如上面这张图片为64像素X64像素，相应地，三个矩阵（RGB）也都是 $64 \\times 64$。\n",
    "\n",
    "矩阵中的值表示着像素中对应通道颜色的强度，三个矩阵中的所有值一起构成了 $n$ 维的特征向量。在模式识别和机器学习中，特征向量表示一个对象，在这个例子中，表示图片包含猫或不包含猫。\n",
    "\n",
    "要创建这个特征向量 $x$，所有的像素颜色强度将被重新整合。特征向量 $x$ 的维度为 $n_x = 64 \\times 64 \\times 3 = 12288$。\n",
    "\n",
    "![feature vector](img/feature vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 逻辑回归\n",
    "\n",
    "逻辑回归是一种解决监督学习中二分类问题的学习算法。逻辑学习的目标是最小化预测值和训练数据标签值之间的误差。\n",
    "\n",
    "依然是训练识别猫的图片的分类器为例，给定一个特征向量 $x$，逻辑回归算法将能够给出图片中包含猫的概率\n",
    "\n",
    "$$\\hat{y} = P(y=1|x), 其中0 \\leq \\hat{y} \\leq 1$$\n",
    "\n",
    "逻辑回归中包含以下参数：\n",
    "\n",
    "- 输入特征向量：$x \\in \\mathbb{R}^{n_x}$，其中 $n_x$ 是特征数\n",
    "- 训练标签：$y \\in \\{0, 1\\}$\n",
    "- 权重：$w \\in \\mathbb{R}^{n_x}$，其中 $n_x$ 是特征数\n",
    "- 阈值：$b \\in \\mathbb{R}$\n",
    "- 输出：$\\hat{y} = \\sigma(w^Tx+b)$\n",
    "- Sigmoid函数：$s = \\sigma(w^Tx+b) = \\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "![sigmoid function](img/sigmoid function.png)\n",
    "\n",
    "$(w^Tx+b)$ 是一个线性函数，由于这里我们需要一个概率表示，输出值需要在 $[0,1]$ 区间内，因此使用了Sigmoid函数。Sigmoid函数的值域为 $[0,1]$，如上图所示。\n",
    "\n",
    "观察Sigmoid函数的图像，可以看到如下一些性质：\n",
    "\n",
    "- 如果 $z$ 是一个非常大的正数，那么 $\\sigma(z)=1$\n",
    "- 如果 $z$ 非常小，或者说是一个非常大的负数，那么 $\\sigma(z)=0$\n",
    "- 如果 $z=0$，那么 $\\sigma(z) = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 逻辑回归的成本函数\n",
    "\n",
    "训练模型获得参数 $w$ 和 $b$，需要首先定义**成本函数 cost function**\n",
    "\n",
    "**损失函数 Loss function**:\n",
    "\n",
    "损失函数衡量着预测值($\\hat{y}^{(i)}$)和期望输出值($y^{(i)}$)之间的差异。换言之，损失函数计算单个训练样本的误差值。\n",
    "\n",
    "$$ L(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "$$ L(\\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)}))$$\n",
    "\n",
    "其中，第一个平方误差的损失函数，会造成非凸的优化目标，逻辑回归通常使用第二个Logloss作为损失函数。\n",
    "\n",
    "**成本函数 Cost function**:\n",
    "\n",
    "成本函数是整个训练集中每个训练样本的损失函数值的均值。最终找到的参数 $w$ 和 $b$ 应该使全局的成本函数取得最小值。\n",
    "\n",
    "$$ J(w, b) = \\frac{1}{m}\\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) = -\\frac{1}{m}\\sum_{i=1}^m[y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)})] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
