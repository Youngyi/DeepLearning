{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度卷积模型：案例分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 案例分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 为什么要进行案例分析\n",
    "\n",
    "过去几年来，计算机视觉领域的主要工作，就在研究如何拼装卷积层、池化层、全连接层等等卷积神经网络模型的基本组件，组合出较为有效的模型，有效的模型通常可以在不同的计算机视觉模型之间通用。因此，通过学习这些模型实例，可以对深度卷积模型有更好的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 经典网络\n",
    "\n",
    "LeNet-5，用来处理灰度图片的数字识别问题。\n",
    "    - 在LeNet-5的年代，补全的技巧还不常用，所有卷积层都不补全\n",
    "    - 池化层在当时更多采用的是平均池化，目前最大池化更为常见\n",
    "    - 模型大约有60000个参数，相对于现代神经网络，规模较小\n",
    "    - 随着模型的深入，高度和宽度不断减小，通道数不断增加，这个实践沿用至今\n",
    "    - 当时的激活函数采用的是sigmoid或者tanh，还没有使用ReLU\n",
    "    - 当时单个过滤器，并不会处理所有的通道，这点和现在也不一样\n",
    "    - 当时在池化层后面会接一个非线性的激活层，和现在不同\n",
    "\n",
    "![LeNet-5.png](img/LeNet-5.png)\n",
    "\n",
    "AlexNet，真正使得计算机视觉领域开始重视神经网络\n",
    "    - 已经开始使用ReLU作为激活函数\n",
    "    - 大约6000万的参数，中等规模\n",
    "    - 当时的GPU计算能力还不够强，论文里涉及两个GPU的通信\n",
    "    - 用到了Local Response Normalization(LRN)的技巧，这个技巧目前已经不常用了\n",
    "    - same padding，同一补全\n",
    "\n",
    "![AlexNet.png](img/AlexNet.png)\n",
    "\n",
    "VGG-16，模式更为明确\n",
    "    - 固定3×3的过滤器，步长s=1，同一补全；最大池化层2×2，步长s=2\n",
    "    - 每次卷积层（连续两层）后，通道数翻倍\n",
    "    - 每次池化层后，高度和宽度减半\n",
    "    - 大约有1.38亿的参数，现代规模的神经网络\n",
    "    - VGG-16中的16是值整个神经网路中共有16层带有参数的层，另外还有一个更大的VGG-19版本，不常用\n",
    "\n",
    "![VGG-16.png](img/VGG-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 残差网络 ResNets\n",
    "\n",
    "由于梯度消失和梯度爆炸问题，训练非常大的神经网络通常非常困难。而残差网络解决了这个问题。残差网络的基本组成结构如下：\n",
    "![Residual block.png](img/Residual block.png)\n",
    "\n",
    "由于计算能力的问题，普通神经网络（Plain Network）在实践中随着层数增多，并不能获得更好的效果。而残差神经网络使训练层数非常多的模型成为可能。\n",
    "![Residual Network.png](img/Residual Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 残差网络为什么有效\n",
    "\n",
    "假设现在有一个 $l$ 层的普通神经网络，输出结果 $a^{[l]}$。我们在其之后，又增加了两层残差Block，得到输出 $a^{[l+2]}$，根据上面的公式，$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$。\n",
    "\n",
    "假设我们对整个网络进行了L2正则化，那么$W^{[l+2]}$ 和 $b^{[l+2]}$ 就会约等于0，使得 $a^{[l+2]}$ 约等于 $a^{[l]}$。也即是说，在最坏的情况下，增加了两层残差Block，也只会使得神经网络的输出相同。而较好的情况下，我们可以训练出更好的模型。\n",
    "\n",
    "值得注意的是，$z^{[l+2]}$ 和 $a^{[l]}$ 要可加，这两个矩阵的维度需要相同。因此，残差网络通常配合着同一补全的卷积层来使用。而对于池化层，则会需要增加一个 $W_s$ 的矩阵，和 $a^{[l]}$ 相乘后再进行加运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 网络中的网络，1×1卷积\n",
    "\n",
    "1×1的卷积在单一通道下看似乎没什么用，但是当通道数多了之后，1×1的卷积实际上可以对跨通道之间的数据项进行非线性组合。\n",
    "![Why does a 1×1 convolution do.png](img/Why does a 1×1 convolution do.png)\n",
    "\n",
    "1×1的卷积还可以对通道数进行缩减（就像池化层对高度和宽度进行缩减），当然如果愿意的化，1×1卷积也可以增加通道数。\n",
    "![Using 1×1 convolutions.png](img/Using 1×1 convolutions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Inception网络的设想\n",
    "\n",
    "避免考虑要用1×1的卷积，还是3×3的卷积，还是池化层。直接将所有这些，叠加到同一层网络中，让模型自己去学习参数。\n",
    "\n",
    "![Motivation for inception network.png](img/Motivation for inception network.png)\n",
    "\n",
    "5×5的卷积，可能会引入比较大的计算量。这时，使用1×1的卷积，在中间做一层瓶颈层，可以有效地降低计算量。\n",
    "\n",
    "![The problem of computational cost.png](img/The problem of computational cost.png)\n",
    "\n",
    "![Using 1×1 convolution.png](img/Using 1×1 convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.7 Inception网络\n",
    "\n",
    "Inception模块使用到了上面的组件，对于3×3和5×5这样的卷积层，会在之前加入一层1×1的卷积作为瓶颈层，减少计算；而对于池化层，除去同一补全之外，还会在之后增加一层1×1的卷积层，用来缩减通道数量。\n",
    "\n",
    "![Inception module.png](img/Inception module.png)\n",
    "\n",
    "Inception网络是由多个Inception模块组合而成的。\n",
    "![Inception network.png](img/Inception network.png)\n",
    "\n",
    "一个小彩蛋，Inception的概念，其实就来自盗梦空间。\n",
    "![WE NEED TO GO DEEPER.png](img/WE NEED TO GO DEEPER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 在实际项目中使用卷积网络的一些建议"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 使用开源实现\n",
    "\n",
    "上面介绍的神经网络架构都比较复杂，在实现的过程中，有很多需要注意的小技巧。对于计算机视觉应用，想要使用上面或其它研究文献中介绍的神经网络架构，通常的建议是去（比如Github上）寻找开源实现，在此基础上进行开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 迁移学习\n",
    "\n",
    "很多开源实现除去实现神经网络架构之外，还会包含该架构在知名数据集（比如ImageNet）上训练完成后的各项权重。直接使用这些权重作为预训练好的权重，在此之上进行迁移学习，而不是重新随机初始化权重从头进行训练，是一个比较好的实践。根据所要解决问题数据量的不同，迁移学习的方案也有几种不同的模式：\n",
    "\n",
    "    - 如果训练集数量很小，建议冻结下载好的权重，在此基础上直接替换增加一层自己的Softmax分类层，只训练这个分类层的权重。冻结权重这个功能，各个框架都有支持；\n",
    "    - 如果训练集数量有中等规模，可以相应地冻结更少层权重，不冻结的几层和自己的Softmax层用来训练；\n",
    "    - 如果训练集数量非常大，可以只用下载的权重替代权重的随机初始化，替换Softmax，整个网络进行训练。\n",
    "\n",
    "计算机视觉领域，迁移学习几乎是必然的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数据扩增\n",
    "\n",
    "对于计算机视觉领域，数据量总是显得不够，数据扩增是对原始数据进行加工，生成新数据的手段。\n",
    "\n",
    "    - 镜像（左右转置），随机剪切（不完美，可能失去图像中的物体，但只要一张图随机剪切的子集够大，在实践中效果也会不错），旋转，修剪（Shearing，不常用），局部翘曲（Local Warping，不常用）。\n",
    "    - 颜色偏移，比如对图片的RGB值进行较小幅度的变更。AlexNet论文中也介绍了应用PCA进行颜色偏移的方法。\n",
    "    \n",
    "实践中在实现时，可能会有一个单独的CPU线程，将扩增后的数据，喂给正常训练的其它CPU线程或者GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4 计算机视觉领域的现状\n",
    "\n",
    "相比问题的复杂度，打标数据少，手动设计的特征工程多，在神经网络的架构设计上比较讲究。\n",
    "\n",
    "![Data vs hand-engineering.png](img/Data vs hand-engineering.png)\n",
    "\n",
    "模型融合（Ensembling）和Multi-crop是两个在比赛中提升成绩的好办法，但生产环境中使用的很少。\n",
    "\n",
    "![Tips for doing well on benchmarks or winning competitions.png](img/Tips for doing well on benchmarks or winning competitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Show me the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Keras教程 - 快乐之家\n",
    "\n",
    "在本节，我们将会学习：\n",
    "1. Python编写的高级神经网络API（编程框架）Keras，它可以跑在多个底层框架上，比如Tensorflow，CNTK。\n",
    "2. 看看我们如何可以在几个小时内就构建一个深度学习算法。\n",
    "\n",
    "开发Keras的目的，就是为了让深度学习工程师可以更快地构建和实验不同的模型。正如Tensorflow相对于Python标准库或者Numpy来说是更高级的框架，Keras是在其之上更高层次的框架，提供了更多的抽象。能够很快地将想法变成结果，对于找到正确的模型来说十分关键。但与此同时，Keras相比底层框架，限制也更多，所以会有一些十分复杂的模型，可以用Tensorflow来表示，但不能（轻易地）用Keras来表示。尽管如此，Keras对于绝大多数常见的模型来说，是十分管用的。\n",
    "\n",
    "在这个练习中，我们会解决“快乐之家”难题，下面会有详细的解析。首先，让我们导入相应的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.1.1 快乐之家\n",
    "\n",
    "下一个假期，你觉得和五位朋友一起过。快乐之家的地理位置十分便利，但最重要的是所有人都承诺，在家的时候会十分开心。所以，想进入房子的人必须证明他们现在十分开心。\n",
    "\n",
    "<img src=\"img/happy-house.jpg\" style=\"width:350px;height:270px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **the Happy House**</center></caption>\n",
    "\n",
    "作为深度学习专家，为了保证“快乐”的原则被严格执行，你会构建一个算法，使用门口摄像头拍摄的图片来判断访客是否快乐。只有在判断访问快乐的时候，门才会自动打开。\n",
    "\n",
    "你收集了用门口摄像头拍摄的一组朋友和自己的照片，数据集已经进行了打标。\n",
    "\n",
    "<img src=\"img/house-members.png\" style=\"width:550px;height:250px;\">\n",
    "\n",
    "执行下面的代码，对数据集进行正规化，并了解数据集的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"快乐之家\"数据集详情**:\n",
    "- 图片维度 (64,64,3)\n",
    "- 训练集: 600 张图片\n",
    "- 测试集: 150 张图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 使用Keras构建模型\n",
    "\n",
    "Keras非常适合用来做快速原型，在非常短的时间内，我们就可以构建出效果非常好的模型。\n",
    "\n",
    "下面是Keras模型的一个例子：\n",
    "```python\n",
    "def model(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "注意到，Keras使用了和Tensorflow或者numpy不太相同的变量命名风格。最主要的一点，它并没有随着前向传播过程创建并赋值一系列诸如 `X`, `Z1`, `A1`, `Z2`, `A2` 的变量。对于不同的层，Keras代码中仅仅是通过 `X = ...` 来重新对 `X` 赋值。换句话说，前向传播中的每个步骤，我们不断地将计算结果重新写回同一个变量 `X`。唯一的例外是 `X_input`，考虑后最后我们需要用 `model = Model(inputs = X_input, ...)` 来创建Keras模型，`X_input` 不会被覆盖。\n",
    "\n",
    "**练习**：实现 `HappyModel()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: HappyModel\n",
    "\n",
    "def HappyModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Feel free to use the suggested outline in the text above to get started, and run through the whole\n",
    "    # exercise (including the later portions of this notebook) once. The come back also try out other\n",
    "    # network architectures as well. \n",
    "        # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建了一个函数，来描述我们的模型。要训练和测试这个模型，在Keras中分为四步：\n",
    "1. 调用上面的函数创建模型。\n",
    "2. 调用 `model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])` 编译模型。\n",
    "3. 调用 `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)` 训练模型。\n",
    "4. 调用 `model.evaluate(x = ..., y = ...)` 测试模型。\n",
    "\n",
    "如果你想了解关于 `model.compile()`, `model.fit()`, `model.evaluate()` 及其参数的更多信息，可以参考官方的[Keras文档](https://keras.io/models/model/).\n",
    "\n",
    "**练习**: 实现步骤1，创建模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "happyModel = HappyModel((X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 实现步骤2，设置学习过程的相关参数以编译模型。请小心地选择 `compile()` 的三个参数。提示：快乐之家是一个二分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "happyModel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 实现步骤3，训练模型。选择epochs和批次大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "600/600 [==============================] - 1s - loss: 0.9612 - acc: 0.7117     \n",
      "Epoch 2/40\n",
      "600/600 [==============================] - 1s - loss: 0.2637 - acc: 0.8967     \n",
      "Epoch 3/40\n",
      "600/600 [==============================] - 1s - loss: 0.1671 - acc: 0.9300     \n",
      "Epoch 4/40\n",
      "600/600 [==============================] - 1s - loss: 0.1410 - acc: 0.9417     \n",
      "Epoch 5/40\n",
      "600/600 [==============================] - 1s - loss: 0.0871 - acc: 0.9733     \n",
      "Epoch 6/40\n",
      "600/600 [==============================] - 1s - loss: 0.0846 - acc: 0.9717     \n",
      "Epoch 7/40\n",
      "600/600 [==============================] - 1s - loss: 0.0964 - acc: 0.9683     \n",
      "Epoch 8/40\n",
      "600/600 [==============================] - 1s - loss: 0.0563 - acc: 0.9817     \n",
      "Epoch 9/40\n",
      "600/600 [==============================] - 1s - loss: 0.1381 - acc: 0.9467     \n",
      "Epoch 10/40\n",
      "600/600 [==============================] - 1s - loss: 0.0879 - acc: 0.9767     \n",
      "Epoch 11/40\n",
      "600/600 [==============================] - 1s - loss: 0.0567 - acc: 0.9817     \n",
      "Epoch 12/40\n",
      "600/600 [==============================] - 1s - loss: 0.0612 - acc: 0.9767     \n",
      "Epoch 13/40\n",
      "600/600 [==============================] - 1s - loss: 0.0524 - acc: 0.9800     \n",
      "Epoch 14/40\n",
      "600/600 [==============================] - 1s - loss: 0.0414 - acc: 0.9917     \n",
      "Epoch 15/40\n",
      "600/600 [==============================] - 1s - loss: 0.0618 - acc: 0.9783     \n",
      "Epoch 16/40\n",
      "600/600 [==============================] - 1s - loss: 0.0836 - acc: 0.9717     \n",
      "Epoch 17/40\n",
      "600/600 [==============================] - 1s - loss: 0.1345 - acc: 0.9583     \n",
      "Epoch 18/40\n",
      "600/600 [==============================] - 1s - loss: 0.1121 - acc: 0.9683     \n",
      "Epoch 19/40\n",
      "600/600 [==============================] - 1s - loss: 0.0508 - acc: 0.9833     \n",
      "Epoch 20/40\n",
      "600/600 [==============================] - 1s - loss: 0.0475 - acc: 0.9800     \n",
      "Epoch 21/40\n",
      "600/600 [==============================] - 1s - loss: 0.0194 - acc: 0.9950     \n",
      "Epoch 22/40\n",
      "600/600 [==============================] - 1s - loss: 0.0368 - acc: 0.9917     \n",
      "Epoch 23/40\n",
      "600/600 [==============================] - 1s - loss: 0.0518 - acc: 0.9817     \n",
      "Epoch 24/40\n",
      "600/600 [==============================] - 1s - loss: 0.0287 - acc: 0.9900     \n",
      "Epoch 25/40\n",
      "600/600 [==============================] - 1s - loss: 0.0414 - acc: 0.9867     \n",
      "Epoch 26/40\n",
      "600/600 [==============================] - 1s - loss: 0.0356 - acc: 0.9867     \n",
      "Epoch 27/40\n",
      "600/600 [==============================] - 1s - loss: 0.0195 - acc: 0.9883     \n",
      "Epoch 28/40\n",
      "600/600 [==============================] - 1s - loss: 0.0407 - acc: 0.9867     \n",
      "Epoch 29/40\n",
      "600/600 [==============================] - 1s - loss: 0.0189 - acc: 0.9933     \n",
      "Epoch 30/40\n",
      "600/600 [==============================] - 1s - loss: 0.0154 - acc: 0.9917     \n",
      "Epoch 31/40\n",
      "600/600 [==============================] - 1s - loss: 0.0079 - acc: 0.9967     \n",
      "Epoch 32/40\n",
      "600/600 [==============================] - 1s - loss: 0.0114 - acc: 0.9950     \n",
      "Epoch 33/40\n",
      "600/600 [==============================] - 1s - loss: 0.0548 - acc: 0.9817     \n",
      "Epoch 34/40\n",
      "600/600 [==============================] - 1s - loss: 0.0358 - acc: 0.9883     \n",
      "Epoch 35/40\n",
      "600/600 [==============================] - 1s - loss: 0.0389 - acc: 0.9833     \n",
      "Epoch 36/40\n",
      "600/600 [==============================] - 1s - loss: 0.0182 - acc: 0.9933     \n",
      "Epoch 37/40\n",
      "600/600 [==============================] - 1s - loss: 0.0064 - acc: 1.0000     \n",
      "Epoch 38/40\n",
      "600/600 [==============================] - 1s - loss: 0.0048 - acc: 0.9983     \n",
      "Epoch 39/40\n",
      "600/600 [==============================] - 1s - loss: 0.0166 - acc: 0.9950     \n",
      "Epoch 40/40\n",
      "600/600 [==============================] - 1s - loss: 0.0331 - acc: 0.9883     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa96c7845f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "happyModel.fit(x=X_train, y=Y_train, epochs=40, batch_size=16)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这时如果重新执行 `fit()`，`model` 会使用之前已经学到的参数继续训练，而不是重新对参数初始化。\n",
    "\n",
    "**练习**: 实现步骤4，即测试/评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s     \n",
      "\n",
      "Loss = 0.235924696922\n",
      "Test Accuracy = 0.92666667064\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = happyModel.evaluate(x=X_test, y=Y_test)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 `happyModel()` 有效的话，这里我们会获得一个远高于随机值（50%）的准确率。\n",
    "\n",
    "这里给一个参考值，**40 epochs，95%的测试准确率**（99%的训练准确率），微批的大小为16，使用adam优化器。\n",
    "\n",
    "如果这里没能取得很好的准确率（80%以上），下面有一些策略可以考虑尝试一下。\n",
    "\n",
    "- 尝试使用 CONV->BATCHNORM->RELU 这样的结构\n",
    "```python\n",
    "X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv0')(X)\n",
    "X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "X = Activation('relu')(X)\n",
    "```\n",
    "直到高度和宽度非常小，而通道数非常大（比如大约32）。这时候，立方体中很多有用的信息都编码在通道维度中。之后可以打平立方体，使用一个全连接层。\n",
    "- 在上面的结构后，使用最大池化层。这可以用来降低高度和宽度的维度。\n",
    "- 修改优化器，我们发现Adam在这个问题中比较好用。\n",
    "- 如果模型跑不起来，用内存问题，尝试降低微批大小（12是一个不错的折中值）\n",
    "- 跑更多轮次（epoch），知道训练集准确率进入平缓状态。\n",
    "\n",
    "即便你的模型准确率已经不错，也还是可以试着修改参数，以获得更好的效果。\n",
    "\n",
    "**注意**: 如果你对模型的超参进行调节，测试集实际上就变成了开发集，你的模型很可能会对测试集（开发集）过拟合。在本练习中，我们暂时不考虑这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Keras中其它一些有用的函数\n",
    "\n",
    "下面有两个Keras中的特性，可能会比较有用：\n",
    "- `model.summary()`: 以表格的形式打印出模型各层输入输出的大小。\n",
    "- `plot_model()`: 打印模型图片，甚至可以使用SVG()函数保存为\".png\"形式。\n",
    "\n",
    "执行下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 70, 70, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "happyModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 残差网络\n",
    "\n",
    "本节我们将使用残差网络（ResNets）来构建非常深的卷积网络。理论上，深度网络可以表示非常复杂的函数，但在实践中，深层的神经网络非常难以训练。[He et al.](https://arxiv.org/pdf/1512.03385.pdf) 发现的残差网络，使得我们可以训练更加深层的神经网络。\n",
    "\n",
    "**在这个练习中，我们将会**\n",
    "- 实现残差网络的基本单元\n",
    "- 将这些基本单元组装起来，实现一个神经网络图片分类器\n",
    "\n",
    "这个练习将通过Keras来实现，首先引入相关的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from resnets_utils import *\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 非常深的神经网络所面临的问题\n",
    "\n",
    "在卷积神经网络基础这一节中，我们构建了卷积神经网络。近些年来，神经网络变得越来越深层，从起初的几层（比如AlexNet）到现在的上百层。\n",
    "\n",
    "神经网络深度加深使我们可以表示更为复杂的函数。模型也可以在不同的抽象层次学习到更多特征，从边（浅层）到非常复杂的特征（深层）。然而，实际中训练更深的神经网络不一定总能产出更好的效果。一个巨大的阻碍在于梯度消失的问题：对于非常深的神经网络，梯度信号很快就会降为0，从而使得梯度下降的过程过于缓慢。具体来说，在梯度下降的过程中，随着反向传播从最终层传播回第一层，每一步都在做矩阵相乘的运算，使得梯度以幂指数级别下降到0。（在某些罕见的情况下，梯度以幂指数级别爆炸上升）\n",
    "\n",
    "因此在训练时，经常可以看到前几层网络梯度的量级很快地下降为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vanishing_grad_kiank.png\" style=\"width:450px;height:220px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Vanishing gradient** <br> The speed of learning decreases very rapidly for the early layers as the network trains </center></caption>\n",
    "\n",
    "我们将通过构建残差网络来解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 构建残差网络\n",
    "\n",
    "在残差网络中，“捷径（shortcut）”或者“跳跃连接（skip connection）”使得梯度可以直接反向传播到较早的层级。\n",
    "\n",
    "<img src=\"img/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : A ResNet block showing a **skip-connection** <br> </center></caption>\n",
    "\n",
    "左侧的图展示的是网络传播的“主要路径（main path）”。右侧的图在主要路径增加了一条“捷径（shortcut）”。通过将多个残差网络单元叠加在一起，我们就可以构建非常深的网络。\n",
    "\n",
    "在上面的课程中也提到，残差网络中的捷径使其非常容易学习同一函数（identity function）。这意味着叠加残差单元，损害之前模型的可能性极小。\n",
    "\n",
    "残差网络单元主要存在两种形式，它们的区别是输入和输出维度相同还是不同。我们会实现这两种残差单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2.1 同一残差单元 The identity block\n",
    "\n",
    "同一残差单元是残差网络中的标准单元，对应着输入激活(比如 $a^{[l]}$) 和输出激活 (比如 $a^{[l+2]}$) 维度相同的情况。下面是同一残差单元的另一种展示形式：\n",
    "\n",
    "<img src=\"img/idblock2_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **Identity block.** Skip connection \"skips over\" 2 layers. </center></caption>\n",
    "\n",
    "上面的路径是“捷径”，下面的路径是“主要路径”。图中我们也显式绘制了每一层中的卷积和激活步骤。为了加速计算，我们还增加了一步批量正则化。\n",
    "\n",
    "而在练习中，我们将会实现一个略强版本的同一残差单元，跳跃连接会跳过3个隐藏层，而不是2个。\n",
    "\n",
    "<img src=\"img/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Identity block.** Skip connection \"skips over\" 3 layers.</center></caption>\n",
    "\n",
    "每一步具体如下：\n",
    "\n",
    "主要路径的第一个组件：\n",
    "- 第一个卷积层 CONV2D 有 $F_1$ 个 (1,1) 的过滤器，步长为 (1,1)。使用\"valid\"补全，名称为`conv_name_base + '2a'`。使用0作为随机数初始化的种子。 \n",
    "- 第一个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2a'`。\n",
    "- 应用 ReLU 激活函数。这一层没有名称或超参。\n",
    "\n",
    "主要路径的第二个组件：\n",
    "- 第二个卷积层 CONV2D 有 $F_2$ 个 (f,f) 的过滤器，步长为 (1,1)。使用\"same\"补全，名称为`conv_name_base + '2b'`。使用0作为随机数初始化的种子。 \n",
    "- 第二个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2b'`。\n",
    "- 应用 ReLU 激活函数。这一层没有名称或超参。\n",
    "\n",
    "主要路径的第三个组件：\n",
    "- 第三个卷积层 CONV2D 有 $F_3$ 个 (1,1) 的过滤器，步长为 (1,1)。使用\"valid\"补全，名称为`conv_name_base + '2c'`。使用0作为随机数初始化的种子。 \n",
    "- 第三个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2c'`。注意这个组件中没有ReLU激活函数。\n",
    "\n",
    "最终步骤: \n",
    "- 捷径和输入累加在一起\n",
    "- 应用ReLU激活函数。没有名称或超参。\n",
    "\n",
    "**练习**: 实现ResNet同一单元。下面是一些参考文档：\n",
    "- 实现 Conv2D 步骤: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
    "- 实现 BatchNorm 步骤: [See reference](https://faroit.github.io/keras-docs/1.2.2/layers/normalization/) (axis: Integer, 需要正则化的坐标 (通常是通道坐标))\n",
    "- 激活可以使用:  `Activation('relu')(X)`\n",
    "- 前向传播和捷径相加: [See reference](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: identity_block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [ 0.94822985  0.          1.16101444  2.747859    0.          1.36677003]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **out**\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.94822985  0.          1.16101444  2.747859    0.          1.36677003]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2.2 卷积残差单元 \n",
    "\n",
    "我们已经实现了同一残差单元，接下来，卷积残差单元处理的是输入和输出维度不匹配的情况。和同一残差单元不同的地方在于，捷径中现在有一层 CONV2D。\n",
    "\n",
    "<img src=\"img/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Convolutional block** </center></caption>\n",
    "\n",
    "捷径中的 CONV2D 层就是用来调整输入 $x$ 的维度，以便最后将捷径的值加回主要路径时，两个矩阵的维度相同。捷径上的卷积层 CONV2D 不附加非线性的激活函数。它主要的用途就是为了之后的可加。\n",
    "\n",
    "卷积残差单元的细节如下：\n",
    "\n",
    "主要路径的第一个组件：\n",
    "- 第一个卷积层 CONV2D 有 $F_1$ 个 (1,1) 的过滤器，步长为 (s,s)。使用\"valid\"补全，名称为`conv_name_base + '2a'`。\n",
    "- 第一个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2a'`。\n",
    "- 应用 ReLU 激活函数。这一层没有名称或超参。\n",
    "\n",
    "主要路径的第二个组件：\n",
    "- 第二个卷积层 CONV2D 有 $F_2$ 个 (f,f) 的过滤器，步长为 (1,1)。使用\"same\"补全，名称为`conv_name_base + '2b'`。\n",
    "- 第二个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2b'`。\n",
    "- 应用 ReLU 激活函数。这一层没有名称或超参。\n",
    "\n",
    "主要路径的第三个组件：\n",
    "- 第三个卷积层 CONV2D 有 $F_3$ 个 (1,1) 的过滤器，步长为 (1,1)。使用\"valid\"补全，名称为`conv_name_base + '2c'`。\n",
    "- 第三个批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '2c'`。注意这个组件中没有ReLU激活函数。\n",
    "\n",
    "捷径：\n",
    "- CONV2D 有 $F_3$ 个 (1,1) 的过滤器，步长为 (s,s)。使用\"valid\"补全，名称为`conv_name_base + '1'`。\n",
    "- 批量正则化 BatchNorm 针对通道坐标进行正则化，名称为 `bn_name_base + '1'`。\n",
    "\n",
    "最终步骤:\n",
    "- 捷径和输入累加在一起\n",
    "- 应用ReLU激活函数。没有名称或超参。\n",
    "\n",
    "**练习**: 实现卷积残差单元。依然使用0作为随机数种子，以保证结果可以浮现。\n",
    "- [Conv Hint](https://keras.io/layers/convolutional/#conv2d)\n",
    "- [BatchNorm Hint](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
    "- For the activation, use:  `Activation('relu')(X)`\n",
    "- [Addition Hint](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convolutional_block\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(F3, (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out = [ 0.09018463  1.23489773  0.46822017  0.0367176   0.          0.65516603]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(1)\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \" + str(out[0][1][1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **out**\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.09018463  1.23489773  0.46822017  0.0367176   0.          0.65516603]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 构建残差网络（50层）\n",
    "\n",
    "这样我们就有了构建非常深的残差网络的基本单元。下面的图详细描述了这个神经网络的架构，\"ID BLOCK\"表示同一残差单元，\"ID BLOCK x3\"表示连续叠加三个同一残差单元。\n",
    "\n",
    "<img src=\"img/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 5** </u><font color='purple'>  : **ResNet-50 model** </center></caption>\n",
    "\n",
    "这个50层的残差网络具体包括：\n",
    "- 使用(3, 3)的零补全对输入补全。\n",
    "- 第一阶段：\n",
    "    - 2D卷积，共64个 (7,7) 的过滤器，步长为 (2,2)。名称叫做\"conv1\"。\n",
    "    - 批量正则化，应用于输入的通道维度\n",
    "    - 最大池化层，(3,3)的窗口，(2,2)的步长\n",
    "- 第二阶段：\n",
    "    - 卷积残差单元，三组过滤器大小分别为[64, 64, 256]，f=3，s=1，单元名称为\"a\"\n",
    "    - 两个同一残差单元，三组过滤器大小分别为[64, 64, 256]，f=3，单元名称为\"b\"和\"c\"\n",
    "- 第三阶段：\n",
    "    - 卷积残差单元，三组过滤器大小分别为[128, 128, 512]，f=3，s=2，单元名称为\"a\"\n",
    "    - 三个同一残差单元，三组过滤器大小分别为[128, 128, 512]，f=3，单元名称为\"b\"，\"c\"，\"d\"\n",
    "- 第四阶段：\n",
    "    - 卷积残差单元，三组过滤器大小分别为[256, 256, 1024]，f=3，s=2，单元名称为\"a\"\n",
    "    - 五个同一残差单元，三组过滤器大小分别为[256, 256, 1024]，f=3，单元名称为\"b\"，\"c\"，\"d\"，\"e\"，\"f\"\n",
    "- 第五阶段：\n",
    "    - 卷积残差单元，三组过滤器大小分别为[512, 512, 2048]，f=3，s=2，单元名称为\"a\"\n",
    "    - 两个同一残差单元，三组过滤器大小分别为[512, 512, 2048]，f=3，单元名称为\"b\"，\"c\"\n",
    "- 2D平均池化层，(2,2)的窗口，名称为\"avg_pool\"\n",
    "- Flatten，打平，没有超参，也没有名称\n",
    "- 全连接层，使用softmax激活函数，将输入转为多元分类数目的输出。名称为'fc' + str(classes)\n",
    "\n",
    "**练习**: 实现上图所描述的50层残差网络。\n",
    "\n",
    "平均池化相关文档: \n",
    "- Average pooling [see reference](https://keras.io/layers/pooling/#averagepooling2d)\n",
    "\n",
    "函数文档参考:\n",
    "- Conv2D: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
    "- BatchNorm: [See reference](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
    "- Zero padding: [See reference](https://keras.io/layers/convolutional/#zeropadding2d)\n",
    "- Max pooling: [See reference](https://keras.io/layers/pooling/#maxpooling2d)\n",
    "- Fully conected layer: [See reference](https://keras.io/layers/core/#dense)\n",
    "- Addition: [See reference](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: ResNet50\n",
    "\n",
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage = 5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage = 5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D((2, 2))(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行下面的代码来构建模型图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这样模型就可以准备开始训练了，让我们载入数据集\n",
    "\n",
    "<img src=\"img/signs_data_kiank.png\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 6** </u><font color='purple'>  : **SIGNS dataset** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1080/1080 [==============================] - 33s - loss: 2.9785 - acc: 0.2472    \n",
      "Epoch 2/100\n",
      "1080/1080 [==============================] - 32s - loss: 1.6969 - acc: 0.5028    \n",
      "Epoch 3/100\n",
      "1080/1080 [==============================] - 32s - loss: 1.3083 - acc: 0.7130    \n",
      "Epoch 4/100\n",
      "1080/1080 [==============================] - 32s - loss: 2.2020 - acc: 0.4519    \n",
      "Epoch 5/100\n",
      "1080/1080 [==============================] - 32s - loss: 1.2940 - acc: 0.6565    \n",
      "Epoch 6/100\n",
      "1080/1080 [==============================] - 32s - loss: 1.1647 - acc: 0.7056    \n",
      "Epoch 7/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.9057 - acc: 0.8111    \n",
      "Epoch 8/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.7449 - acc: 0.7806    \n",
      "Epoch 9/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.4622 - acc: 0.8583    \n",
      "Epoch 10/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.7148 - acc: 0.7944    \n",
      "Epoch 11/100\n",
      "1080/1080 [==============================] - 32s - loss: 1.5208 - acc: 0.5880    \n",
      "Epoch 12/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.8318 - acc: 0.7333    \n",
      "Epoch 13/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.9559 - acc: 0.7167    \n",
      "Epoch 14/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.6172 - acc: 0.7898    \n",
      "Epoch 15/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.3044 - acc: 0.9000    \n",
      "Epoch 16/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.1944 - acc: 0.9352    \n",
      "Epoch 17/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.1132 - acc: 0.9574    \n",
      "Epoch 18/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.1130 - acc: 0.9657    \n",
      "Epoch 19/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0755 - acc: 0.9750    \n",
      "Epoch 20/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0806 - acc: 0.9639    \n",
      "Epoch 21/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.1159 - acc: 0.9574    \n",
      "Epoch 22/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0644 - acc: 0.9741    \n",
      "Epoch 23/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0501 - acc: 0.9815    \n",
      "Epoch 24/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0873 - acc: 0.9694    \n",
      "Epoch 25/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0565 - acc: 0.9796    \n",
      "Epoch 26/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0299 - acc: 0.9907    \n",
      "Epoch 27/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0338 - acc: 0.9907    \n",
      "Epoch 28/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0315 - acc: 0.9935    \n",
      "Epoch 29/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0173 - acc: 0.9954    \n",
      "Epoch 30/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0251 - acc: 0.9926    \n",
      "Epoch 31/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0185 - acc: 0.9944    \n",
      "Epoch 32/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0274 - acc: 0.9889    \n",
      "Epoch 33/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0428 - acc: 0.9833    \n",
      "Epoch 34/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0601 - acc: 0.9796    \n",
      "Epoch 35/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0861 - acc: 0.9750    \n",
      "Epoch 36/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0313 - acc: 0.9907    \n",
      "Epoch 37/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0151 - acc: 0.9954    \n",
      "Epoch 38/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0316 - acc: 0.9926    \n",
      "Epoch 39/100\n",
      "1080/1080 [==============================] - 43s - loss: 0.0279 - acc: 0.9917    \n",
      "Epoch 40/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.0441 - acc: 0.9852    \n",
      "Epoch 41/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0490 - acc: 0.9833    \n",
      "Epoch 42/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.3852 - acc: 0.8926    \n",
      "Epoch 43/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.2291 - acc: 0.9296    \n",
      "Epoch 44/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0984 - acc: 0.9648    \n",
      "Epoch 45/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0581 - acc: 0.9833    \n",
      "Epoch 46/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0217 - acc: 0.9954    \n",
      "Epoch 47/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0075 - acc: 0.9991    \n",
      "Epoch 48/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0493 - acc: 0.9833    \n",
      "Epoch 49/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0426 - acc: 0.9852    \n",
      "Epoch 50/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0578 - acc: 0.9926    \n",
      "Epoch 51/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0329 - acc: 0.9907    \n",
      "Epoch 52/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0369 - acc: 0.9944    \n",
      "Epoch 53/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0442 - acc: 0.9870    \n",
      "Epoch 54/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0197 - acc: 0.9944    \n",
      "Epoch 55/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0090 - acc: 0.9963    \n",
      "Epoch 56/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0076 - acc: 0.9972    \n",
      "Epoch 57/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0280 - acc: 0.9898    \n",
      "Epoch 58/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0355 - acc: 0.9917    \n",
      "Epoch 59/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0473 - acc: 0.9907    \n",
      "Epoch 60/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0060 - acc: 0.9972    \n",
      "Epoch 61/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0090 - acc: 0.9963    \n",
      "Epoch 62/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0030 - acc: 1.0000    \n",
      "Epoch 63/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0341 - acc: 0.9972    \n",
      "Epoch 64/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0044 - acc: 0.9991    \n",
      "Epoch 65/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0016 - acc: 1.0000        \n",
      "Epoch 66/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0014 - acc: 1.0000    \n",
      "Epoch 67/100\n",
      "1080/1080 [==============================] - 33s - loss: 7.3010e-04 - acc: 1.0000    \n",
      "Epoch 68/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0022 - acc: 0.9981    \n",
      "Epoch 69/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0166 - acc: 0.9981    \n",
      "Epoch 70/100\n",
      "1080/1080 [==============================] - 32s - loss: 7.3046e-04 - acc: 1.0000    \n",
      "Epoch 71/100\n",
      "1080/1080 [==============================] - 32s - loss: 4.6707e-04 - acc: 1.0000    \n",
      "Epoch 72/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0276 - acc: 0.9963    \n",
      "Epoch 73/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.1378 - acc: 0.9593    \n",
      "Epoch 74/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0830 - acc: 0.9704    \n",
      "Epoch 75/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0679 - acc: 0.9769    \n",
      "Epoch 76/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0423 - acc: 0.9870    \n",
      "Epoch 77/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0139 - acc: 0.9954    \n",
      "Epoch 78/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0266 - acc: 0.9907    \n",
      "Epoch 79/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.0454 - acc: 0.9935    \n",
      "Epoch 80/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0141 - acc: 0.9926    \n",
      "Epoch 81/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0513 - acc: 0.9898    \n",
      "Epoch 82/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0138 - acc: 0.9963    \n",
      "Epoch 83/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0165 - acc: 0.9963    \n",
      "Epoch 84/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.0017 - acc: 1.0000    \n",
      "Epoch 85/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.1255 - acc: 0.9843    \n",
      "Epoch 86/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.8047 - acc: 0.8704    \n",
      "Epoch 87/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.9964 - acc: 0.6898    \n",
      "Epoch 88/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.3829 - acc: 0.8759    \n",
      "Epoch 89/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.2379 - acc: 0.9222    \n",
      "Epoch 90/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.1551 - acc: 0.9435    \n",
      "Epoch 91/100\n",
      "1080/1080 [==============================] - 35s - loss: 0.1009 - acc: 0.9769    \n",
      "Epoch 92/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.1194 - acc: 0.9611    \n",
      "Epoch 93/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.1495 - acc: 0.9500    \n",
      "Epoch 94/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0495 - acc: 0.9852    \n",
      "Epoch 95/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0297 - acc: 0.9907    \n",
      "Epoch 96/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0666 - acc: 0.9907    \n",
      "Epoch 97/100\n",
      "1080/1080 [==============================] - 34s - loss: 0.0170 - acc: 0.9926    \n",
      "Epoch 98/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0129 - acc: 0.9963    \n",
      "Epoch 99/100\n",
      "1080/1080 [==============================] - 32s - loss: 0.0406 - acc: 0.9861    \n",
      "Epoch 100/100\n",
      "1080/1080 [==============================] - 33s - loss: 0.0471 - acc: 0.9852    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa96c070eb8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看一下训练后的模型，在测试集上表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s     \n",
      "Loss = 0.448493123055\n",
      "Test Accuracy = 0.900000003974\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 70, 70, 3)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 32, 32, 64)    9472        zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, 32, 32, 64)    256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 64)    0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 15, 15, 64)    0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 15, 15, 64)    4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 15, 15, 64)    0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 15, 15, 64)    0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 15, 15, 256)   16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, 15, 15, 256)   1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 15, 15, 256)   0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 15, 15, 256)   0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 15, 15, 64)    16448       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 15, 15, 64)    0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 15, 15, 64)    0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 15, 15, 256)   0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 15, 15, 256)   0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 15, 15, 64)    16448       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 15, 15, 64)    0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 15, 15, 64)    0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 15, 15, 256)   0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 15, 15, 256)   0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 8, 8, 128)     32896       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 8, 8, 128)     0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 8, 8, 128)     0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 8, 8, 512)     131584      activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, 8, 8, 512)     2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 8, 8, 512)     0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 8, 8, 512)     0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 8, 8, 128)     0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 8, 8, 128)     0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 8, 8, 512)     0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 8, 8, 512)     0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 8, 8, 128)     0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 8, 8, 128)     0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 8, 8, 512)     0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 8, 8, 512)     0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 8, 8, 128)     0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 8, 8, 128)     0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 8, 8, 512)     0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 8, 8, 512)     0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 4, 4, 256)     131328      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 4, 4, 256)     0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 4, 4, 256)     0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 4, 4, 1024)    525312      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, 4, 4, 1024)    4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 4, 4, 1024)    0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 4, 4, 1024)    0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 4, 4, 256)     0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 4, 4, 256)     0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 4, 4, 1024)    0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 4, 4, 1024)    0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 4, 4, 256)     0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 4, 4, 256)     0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 4, 4, 1024)    0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 4, 4, 1024)    0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 4, 4, 256)     0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 4, 4, 256)     0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 4, 4, 1024)    0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 4, 4, 1024)    0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 4, 4, 256)     0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 4, 4, 256)     0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 4, 4, 1024)    0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 4, 4, 1024)    0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 4, 4, 256)     0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 4, 4, 256)     0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 4, 4, 1024)    0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 4, 4, 1024)    0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 2, 2, 512)     524800      activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 2, 2, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 2, 2, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 2, 2, 2048)    2099200     activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, 2, 2, 2048)    8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 2, 2, 2048)    0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 2, 2, 2048)    0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 2, 2, 512)     1049088     activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 2, 2, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 2, 2, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 2, 2, 2048)    0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 2, 2, 2048)    0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 2, 2, 512)     1049088     activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 2, 2, 512)     0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 2, 2, 512)     0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_51[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_17 (Add)                     (None, 2, 2, 2048)    0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 2, 2, 2048)    0           add_17[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 1, 1, 2048)    0           activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2048)          0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "fc6 (Dense)                      (None, 6)             12294       flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 23,600,006\n",
      "Trainable params: 23,546,886\n",
      "Non-trainable params: 53,120\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**总结**\n",
    "- 非常深的普通神经网络在实践中效果很差，原因是梯度消失\n",
    "- 跳跃连接的技巧，一定程度解决了梯度消失的问题。同时，它也使得残差单元很容易学得同一函数。\n",
    "- 常见的残差单元有两种：同一残差单元和卷积残差单元\n",
    "- 非常深的残差网络，是通过叠加多个残差单元来构建的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献：\n",
    "\n",
    "这个练习主要展示了由 He et al. (2015) 提出的残差网络算法，算法的实现参考了Francois Chollet在Github上开源的代码实现。\n",
    "\n",
    "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
    "- Francois Chollet's github repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
