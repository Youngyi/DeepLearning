{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度卷积模型：案例分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 案例分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 为什么要进行案例分析\n",
    "\n",
    "过去几年来，计算机视觉领域的主要工作，就在研究如何拼装卷积层、池化层、全连接层等等卷积神经网络模型的基本组件，组合出较为有效的模型，有效的模型通常可以在不同的计算机视觉模型之间通用。因此，通过学习这些模型实例，可以对深度卷积模型有更好的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 经典网络\n",
    "\n",
    "LeNet-5，用来处理灰度图片的数字识别问题。\n",
    "    - 在LeNet-5的年代，补全的技巧还不常用，所有卷积层都不补全\n",
    "    - 池化层在当时更多采用的是平均池化，目前最大池化更为常见\n",
    "    - 模型大约有60000个参数，相对于现代神经网络，规模较小\n",
    "    - 随着模型的深入，高度和宽度不断减小，通道数不断增加，这个实践沿用至今\n",
    "    - 当时的激活函数采用的是sigmoid或者tanh，还没有使用ReLU\n",
    "    - 当时单个过滤器，并不会处理所有的通道，这点和现在也不一样\n",
    "    - 当时在池化层后面会接一个非线性的激活层，和现在不同\n",
    "\n",
    "![LeNet-5.png](img/LeNet-5.png)\n",
    "\n",
    "AlexNet，真正使得计算机视觉领域开始重视神经网络\n",
    "    - 已经开始使用ReLU作为激活函数\n",
    "    - 大约6000万的参数，中等规模\n",
    "    - 当时的GPU计算能力还不够强，论文里涉及两个GPU的通信\n",
    "    - 用到了Local Response Normalization(LRN)的技巧，这个技巧目前已经不常用了\n",
    "    - same padding，同一补全\n",
    "\n",
    "![AlexNet.png](img/AlexNet.png)\n",
    "\n",
    "VGG-16，模式更为明确\n",
    "    - 固定3×3的过滤器，步长s=1，同一补全；最大池化层2×2，步长s=2\n",
    "    - 每次卷积层（连续两层）后，通道数翻倍\n",
    "    - 每次池化层后，高度和宽度减半\n",
    "    - 大约有1.38亿的参数，现代规模的神经网络\n",
    "    - VGG-16中的16是值整个神经网路中共有16层带有参数的层，另外还有一个更大的VGG-19版本，不常用\n",
    "\n",
    "![VGG-16.png](img/VGG-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.3 残差网络 ResNets\n",
    "\n",
    "由于梯度消失和梯度爆炸问题，训练非常大的神经网络通常非常困难。而残差网络解决了这个问题。残差网络的基本组成结构如下：\n",
    "![Residual block.png](img/Residual block.png)\n",
    "\n",
    "由于计算能力的问题，普通神经网络（Plain Network）在实践中随着层数增多，并不能获得更好的效果。而残差神经网络使训练层数非常多的模型成为可能。\n",
    "![Residual Network.png](img/Residual Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 残差网络为什么有效\n",
    "\n",
    "假设现在有一个 $l$ 层的普通神经网络，输出结果 $a^{[l]}$。我们在其之后，又增加了两层残差Block，得到输出 $a^{[l+2]}$，根据上面的公式，$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$。\n",
    "\n",
    "假设我们对整个网络进行了L2正则化，那么$W^{[l+2]}$ 和 $b^{[l+2]}$ 就会约等于0，使得 $a^{[l+2]}$ 约等于 $a^{[l]}$。也即是说，在最坏的情况下，增加了两层残差Block，也只会使得神经网络的输出相同。而较好的情况下，我们可以训练出更好的模型。\n",
    "\n",
    "值得注意的是，$z^{[l+2]}$ 和 $a^{[l]}$ 要可加，这两个矩阵的维度需要相同。因此，残差网络通常配合着同一补全的卷积层来使用。而对于池化层，则会需要增加一个 $W_s$ 的矩阵，和 $a^{[l]}$ 相乘后再进行加运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 网络中的网络，1×1卷积\n",
    "\n",
    "1×1的卷积在单一通道下看似乎没什么用，但是当通道数多了之后，1×1的卷积实际上可以对跨通道之间的数据项进行非线性组合。\n",
    "![Why does a 1×1 convolution do.png](img/Why does a 1×1 convolution do.png)\n",
    "\n",
    "1×1的卷积还可以对通道数进行缩减（就像池化层对高度和宽度进行缩减），当然如果愿意的化，1×1卷积也可以增加通道数。\n",
    "![Using 1×1 convolutions.png](img/Using 1×1 convolutions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
