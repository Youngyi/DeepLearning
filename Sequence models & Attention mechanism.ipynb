{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列模型和注意机制 Sequence Models & Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 常见的序列到序列架构 Various sequence to sequence architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 基本模型 Basic Models\n",
    "\n",
    "![Sequence to sequence model.png](img/Sequence to sequence model.png)\n",
    "\n",
    "![Image captioning.png](img/Image captioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 挑选概率最高的句子 Picking the most likely sentence\n",
    "\n",
    "序列到序列的机器翻译模型，和语言模型，有一定相同的地方，它们都需要挑选概率最高的句子。但二者之间也存在一些显著的不同。\n",
    "\n",
    "序列到序列的机器翻译模型，可以看做条件概率下的语言模型：\n",
    "![Machine translation as building a conditional language model.png](img/Machine translation as building a conditional language model.png)\n",
    "\n",
    "有了这个条件概率的语言模型，它可以生成不同的翻译，并且判定各个翻译的概率。但对于机器翻译问题，我们不需要随机生成的句子，而是需要找到概率最大的句子：\n",
    "![Finding the most likely translation.png](img/Finding the most likely translation.png)\n",
    "\n",
    "使用贪心算法，每一步找到最大概率的下一个词，可能导致无法找到全局最大值；而穷举搜索是不现实的，计算量太大：\n",
    "![Why not a greedy search.png](img/Why not a greedy search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 定向搜索 Beam Search\n",
    "\n",
    "定向搜索不一定保证能找到全局最优解，但通常能达到不错的效果。\n",
    "\n",
    "第一步：预先设定好**定向宽度 beam width**的参数B，计算encoder部分的值 $x$，根据 $x$ 就可以计算 $P(y^{<1>}|x)$ 的值。取条件概率最大的B个词，并保存其对应的条件概率。\n",
    "\n",
    "![Beam search algorithm 1.png](img/Beam search algorithm 1.png)\n",
    "\n",
    "第二步：将上一步选出的B个词，分别作为 $y^{<1>}$，将词典中所有的词作为 $y^{<2>}$，计算 $P(y^{<1>},y^{<2>}|x) = P(y^{<1>}|x)P(y^{<2>}|x, y^{<1>})$。这样假设词典中有10000个词，就可以获得 10000×B 组 $y^{<1>}，y^{<1>}$ 的组合。再一次从中挑选出概率最大的B个 $y^{<1>}，y^{<1>}$ 组合。\n",
    "\n",
    "![Beam search algorithm 2.png](img/Beam search algorithm 2.png)\n",
    "\n",
    "第三步：类似上一步。继续重复上面的步骤，知道句子结束。注意当 $B=1$ 时，定向搜索就是贪心搜索。\n",
    "\n",
    "![Beam search algorithm 3.png](img/Beam search algorithm 3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 改进定向搜索 Refinements to Beam Search\n",
    "\n",
    "长度归一化：多个概率值相乘，可能导致堆栈下溢，丢失精度。转为求对数的最大值。然后求平均，从而对长度进行归一，否则之前的优化目标会对长句子有过多的惩罚。\n",
    "\n",
    "![Length normalization.png](img/Length normalization.png)\n",
    "\n",
    "![Beam search discussion.png](img/Beam search discussion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.5 定向搜索的误差分析 Error analysis in beam search\n",
    "\n",
    "误差分析，定向搜索出来的结果不好，应该归因为定向搜索，还是归因为RNN模型。\n",
    "\n",
    "![Error analysis beam search.png](img/Error analysis beam search.png)\n",
    "\n",
    "![Error analysis on beam search.png](img/Error analysis on beam search.png)\n",
    "\n",
    "从训练集中找出所有翻译错误，和人工翻译进行比较。判定定向搜索和RNN模型各自对造成最终误差的占比。如果是RNN，可以再进一步分析，是增加正则化，获取更多数据，还是更换神经网络的架构。\n",
    "\n",
    "![Error analysis process.png](img/Error analysis process.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.6 BLEU评分\n",
    "\n",
    "[BLEU: a Method for Automatic Evaluation of Machine Translation](http://www.aclweb.org/anthology/P02-1040)\n",
    "\n",
    "正确的翻译可能有很多种，要在多个正确的翻译之间取舍，就需要一个评分机制。\n",
    "\n",
    "![Bleu score on unigrams.png](img/Bleu score on unigrams.png)\n",
    "\n",
    "![Bleu details.png](img/Bleu details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.7 注意力模型的直观理解 Attention model intuition\n",
    "\n",
    "目前介绍的机器翻译模型，是编码解码架构，编码和解码分别训练两个不同的RNN，整个句子通过编码RNN输出一个预测值，然后根据这个预测值，解码RNN将其翻译出来。而实际上人类在进行翻译的时候，尤其对于长句子，不会整个看完之后再开始翻译。因为这种传统架构，随着句子变长，其Bleu评分会缓缓下降。\n",
    "\n",
    "![The problem of long sequences.png](img/The problem of long sequences.png)\n",
    "\n",
    "[NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "![Attention model intuition.png](img/Attention model intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 注意力模型 Attention model\n",
    "\n",
    "![Attention model.png](img/Attention model.png)\n",
    "\n",
    "![Computing attention.png](img/Computing attention.png)\n",
    "\n",
    "![Attention examples.png](img/Attention examples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 语音数据 Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 语音识别 Speech Recognition\n",
    "\n",
    "![Speech recognition problem.png](img/Speech recognition problem.png)\n",
    "\n",
    "![Attention model for speech recognition.png](img/Attention model for speech recognition.png)\n",
    "\n",
    "![CTC cost for speech recognition.png](img/CTC cost for speech recognition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 触发字检测 Trigger Word Detection\n",
    "\n",
    "![What is trigger word detection.png](img/What is trigger word detection.png)\n",
    "\n",
    "![Trigger word detection algorithm.png](img/Trigger word detection algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
