{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 浅层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 神经网络总览\n",
    "\n",
    "在上一节我们介绍了逻辑回归的计算过程，将多个逻辑回归的sigmoid单元融合起来，就成为了一个非常简单的神经网络。\n",
    "\n",
    "下图中，每个圆圈所代表的神经元，包含了线性组合（$w^Tx+b$）以及激活函数激活（$\\sigma(z)$）两个计算步骤。\n",
    "\n",
    "为了方便计算，在神经网络中，每一层的权重会形成一个矩阵 $W$（而不再是向量 $w$），一个截距向量 $b$（而不再是截距标量 $b$）。用方括号上标 $[i]$ 表示第 $i$ 层对应的参数。习惯上，输入层不计入神经网络的层数（也可以把输入层看做是第 $0$ 层）。下图表示了一个两层的神经网络及其对应的参数。\n",
    "\n",
    "整个计算过程，也是往复的前向传播和后向传播过程。\n",
    "\n",
    "![What is a neural network](img/What is a Neural Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. 神经网络的表示\n",
    "\n",
    "输入层、隐藏层、输出层。在监督学习的环境中，输入层和输出层的数据是已有的，而隐藏层是无法观测到的变量。每一层的值可以用 $a^{[l]}$ 来表示，$a$ 代表激活activation，是本层的输出，同时也是给定到下一层的输入。模型的输入 $X$ 可以看做是 $a^{[0]}$。\n",
    "\n",
    "![Neural Network Representation](img/Neural Network Representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 神经网络的计算过程\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面说到，逻辑回归的过程，其实就是一个神经元，包含线性组合和激活函数激活两个步骤。而神经网络的计算过程，可以看做是重复计算多次逻辑回归。\n",
    "\n",
    "![Computing a Neural Network's Output 1](img/Computing a Neural Network's Output 1.png)\n",
    "![Computing a Neural Network's Output 2](img/Computing a Neural Network's Output 2.png)\n",
    "\n",
    "重复计算的过程，如果使用for循环来写，会十分低效。事实上，将每一层每个神经元的权重向量组合为权重矩阵，将截距标量组合为截距向量，可以大幅简化计算。\n",
    "![Computing a Neural Network's Output 3](img/Computing a Neural Network's Output 3.png)\n",
    "\n",
    "在这个两层的神经网络中，完成对隐藏层的计算后，最终计算输出层，就又回到了一个四个特征的逻辑回归模型。\n",
    "![Computing a Neural Network's Output 4](img/Computing a Neural Network's Output 4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 针对多个训练样本的向量化\n",
    "\n",
    "在上面的图中，$x_1$, $x_2$ 等都是标量，也即单个实数。当有 $m$ 个训练样本时，上面的计算可能需要重复计算 $m$ 次。但通过将 $x_1$ 转为一个 $m$ 维的向量，将 $[x_1, x_2, x_3]$ 向量转为一个 $n \\times m$ 的矩阵，其他保持不变，可以避免for循环。\n",
    "\n",
    "![Vectorizing across multiple examples](img/Vectorizing across multiple examples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
