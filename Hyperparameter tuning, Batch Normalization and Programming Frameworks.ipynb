{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参调节、批量标准化、编程框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 超参调节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 调参过程\n",
    "\n",
    "调参的优先级：\n",
    "\n",
    "1）学习速率 $\\alpha$\n",
    "\n",
    "2）如果使用动量梯度下降 $\\beta$；隐藏神经元数量；微批大小。\n",
    "\n",
    "3）神经网络的层数；学习速率衰减\n",
    "\n",
    "4）如果使用Adam，几乎不太会去调 $\\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8}$\n",
    "\n",
    "在机器学习的早期阶段，通常会使用网格搜索。当超参较少的情况下，网格搜索是没问题的。对于神经网络这样超参众多的模型，更推荐随机挑选。因为通常很难预先知道哪一个超参会更重要，实际上超参发挥的作用，会有很大的差距，会重要的超参尝试更多的值搜索效果会更好。\n",
    "\n",
    "![Try random values, don't use a grid](img/Try random values, don't use a grid.png)\n",
    "\n",
    "另一个推荐的搜索策略，是采用由粗到细的搜索（coarse to fine），在表现较好的超参区域，放大，进一步搜索。\n",
    "\n",
    "![Coarse to fine](img/Coarse to fine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
