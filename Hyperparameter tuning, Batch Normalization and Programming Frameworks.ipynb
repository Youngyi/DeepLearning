{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参调节、批量标准化、编程框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 超参调节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 调参过程\n",
    "\n",
    "调参的优先级：\n",
    "\n",
    "1）学习速率 $\\alpha$\n",
    "\n",
    "2）如果使用动量梯度下降 $\\beta$；隐藏神经元数量；微批大小。\n",
    "\n",
    "3）神经网络的层数；学习速率衰减\n",
    "\n",
    "4）如果使用Adam，几乎不太会去调 $\\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8}$\n",
    "\n",
    "在机器学习的早期阶段，通常会使用网格搜索。当超参较少的情况下，网格搜索是没问题的。对于神经网络这样超参众多的模型，更推荐随机挑选。因为通常很难预先知道哪一个超参会更重要，实际上超参发挥的作用，会有很大的差距，会重要的超参尝试更多的值搜索效果会更好。\n",
    "\n",
    "![Try random values, don't use a grid](img/Try random values, don't use a grid.png)\n",
    "\n",
    "另一个推荐的搜索策略，是采用由粗到细的搜索（coarse to fine），在表现较好的超参区域，放大，进一步搜索。\n",
    "\n",
    "![Coarse to fine](img/Coarse to fine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 使用合适的刻度来搜索超参\n",
    "\n",
    "上一节提到，随机挑选超参可以提高超参搜索的效率，但与此同时，随机抽样并不意味着随机均匀抽样。当超参搜索的范围量级变动较大时，使用合适的刻度来随机抽样十分重要。\n",
    "\n",
    "![Appropriate scale for hyperparameters](img/Appropriate scale for hyperparameters.png)\n",
    "\n",
    "![Hyperparameters for exponentially weighted averages](img/Hyperparameters for exponentially weighted averages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 超参调节实践：熊猫策略 vs 鱼子酱策略\n",
    "\n",
    "不同类型的问题，超参通常都是重新调节的；即使对于同一个问题，随着时间推移，新的数据进入训练集，时不时对模型表现进行评估并重新搜索超参也是十分必要的。\n",
    "\n",
    "取决于所拥有的计算资料，调节超参也包含串行和并行两种策略。\n",
    "\n",
    "![Pandas vs Caviar](img/Pandas vs Caviar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 批量标准化\n",
    "\n",
    "### 2.1 对神经网络中的激活值进行标准化\n",
    "\n",
    "批量标准化可以使超参搜索更容易，使神经网络模型更健壮，也使得训练更深的神经网络更为容易。批量标准化的起源，是当我们对输入进行标准化后，训练过程会更快。基于同样的原因，如果对每个隐藏层的激活值也进行标准化，那么训练过程会不会进一步加快？但在技术实现中，**实际情况更多是对线性组合的Z值进行标准化，而非激活值**。\n",
    "\n",
    "![Normalizing inputs to speed up learning](img/Normalizing inputs to speed up learning.png)\n",
    "\n",
    "批量标准化，并不像对输入的处理，将其都标准化为均值0，标准差1，而是允许不同的均值和标准差。这个是通过 $\\gamma$ 和 $\\beta$ 这两个值来控制的，这两个值像权重一样，是模型可以学习得到的。\n",
    "\n",
    "![Implementing Batch Norm](img/Implementing Batch Norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 将批量标准化应用到神经网络中\n",
    "\n",
    "![Adding Batch Norm to a network](img/Adding Batch Norm to a network.png)\n",
    "\n",
    "批量标准化通常和微批梯度下降一起使用。另外，在使用了批量标准化之后，参数 $b$ 实际上可以去除了。\n",
    "\n",
    "![Working with mini-batches](img/Working with mini-batches.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
